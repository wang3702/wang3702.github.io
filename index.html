<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
          <meta charset="UTF-8">
          <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
          <meta http-equiv="x-ua-compatible" content="ie=edge">

        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="description" content="" />
        <meta name="keywords" content="" />
        <title>Haoqi Fan</title>
        <link rel="stylesheet" type="text/css" href="css/style.css" />


          <link rel="stylesheet" href="css/bootstrap.min.css">
          <link rel="stylesheet" href="css/academicons.min.css">
          <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
          <link rel="stylesheet" href="css/font-awesome.min.css">


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville%7CMontserrat:400,700%7CRoboto%7CRoboto Mono&display=swap">
    <link rel="stylesheet" href="/css/academic.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">

            <style>
            a.plain-black {
              text-decoration: none;
              color: black;
              margin: 5px;
            }
          </style>
    </head>
    <body>
    


        <div id="wrapper">

            <div id="inner">
                <div id="page">
                    <div id="content">
                            <table>
                            <tr>
                                <td align="left" width = "180"><img class="image" src="image/Xiao_Wang.jpeg" width = "135px" alt="" /></td>                                <td>
                                <h2>Xiao Wang</h2>
                                <p> Ph.D Candidate <br>
                                    <a href="https://purdue.edu" target="_blank">Purdue University</a> <br>
                                    E-mail: wang3702 [at] purdue [dot] edu <br>
                                    <a class="plain-black" href="mailto:wang3702@purdue.edu"><i class="fa fa-2x fa-envelope"></i></a>
									<a class="plain-black" href="https://scholar.google.com/citations?user=AGS_dK8AAAAJ&hl=en" target="_blank"><i class="ai ai-google-scholar-square ai-2x"></i></a>
                                 </p>
                                 </td>
                                <td>
                            </tr>
                            </table>
                            <br/>
                            <h3>About Me</h3>
                            <table>
                            <tr>
                                <td align="left" bgcolor="" width = "1000px",  line-height = "120px" >
                                <p>
Xiao Wang is currently a Computer Science Ph.D student in  <a href="https://kiharalab.org" target="_blank">Kihara Lab</a>, <a href="https://purdue.edu" target="_blank">Purdue University</a>, advised by Prof. <a href="https://kiharalab.org/" target="_blank">Daisuke Kihara</a>. His research interests lie in representation learning for computer vision, bioinformatics and all other intelligent systems.
                                </p>
                                <p>
Starting from 2018, he mainly worked with Prof. <a href="https://kiharalab.org/" target="_blank">Daisuke Kihara</a> on the representation learning for protein structure detection and evaluation. In summer 2018, he did an internship in <a href="https://www.futurewei.com/" target="_blank">Futurewei AI Lab</a> supervised by Dr. <a href="https://sites.google.com/site/gggchenlin/home" target="_blank">Lin Chen</a>, Prof. <a href="https://www.eecs.ucf.edu/~gqi/" target="_blank">Guo-Jun Qi</a> and Prof. <a href="https://www.cs.rochester.edu/u/jluo/" target="_blank">Jiebo Luo</a>. In summer 2019, he did internship in <a href="https://corporate.jd.com/home" target="_blank">JD AI Research</a> supervised by Dr. <a href="http://www.cs.ucf.edu/~liujg/" target="_blank">Jingen Liu</a>. In summer 2020, he did internship in <a href="https://ai.facebook.com/" target="_blank">Facebook AI Research</a> supervised by Dr.  <a href="https://xinleic.xyz/" target="_blank">Xinlei Chen</a> and  <a href="https://haoqifan.github.io/" target="_blank">Haoqi Fan</a>.
                                </p>
                                <p>
He graduated with a bachelor's degree in computer science from <a href="http://en.xjtu.edu.cn/" target="_blank">Xi'an Jiaotong University</a>, Xi'an, China. During his undergraduate, he mainly worked on representation learning for intelligent transportation systems under the supervision of Prof. <a href="https://www.au.tsinghua.edu.cn/info/1076/1605.htm" target="_blank">Li Li</a> from <a href="https://www.tsinghua.edu.cn/en/" target="_blank">Tsinghua University</a> and Prof.<a href="http://people.ucas.ac.cn/~wangfeiyue?language=en" target="_blank">Fei-Yue Wang</a> from  <a href="http://www.compsys.ia.ac.cn/EN/index.html" target="_blank">State Key Laboratory of Management and Control for Complex Systems</a> . He was a summer intern at Purdue in 2017, working on protein model evaluation supervised by Prof. <a href="https://kiharalab.org/" target="_blank">Daisuke Kihara</a>.
                                </p>
                                </td>
                              </tr>
                            </table>

                            <h3>Recent News</h3>


                            <! –– DAQ ––>
                            <table>
                            <tr>
                                <td align="left" bgcolor="" width = "1000px" >
                                    <li> <b>Dec 2021</b>: Released <a href = "https://github.com/kiharalab/DAQ" target="_blank">DAQ</a> - A deep learning based tool for residue-wise local quality for protein models from cryo-Electron Microscopy (EM) maps.
&nbsp;<a href="https://github.com/kiharalab/DAQ"><img alt="GitHub stars" src="https://img.shields.io/github/stars/kiharalab/DAQ?style=social"></a>
                                    </li>
                                    <br/>
                                </td>
                              </tr>
                            </table>




                            <! –– AdCo acceptance––>
                            <table>
                            <tr>
                                <td align="left" bgcolor="" width = "1000px" >
                                    <li> <b>March 2021</b>: "AdCo: Adversarial Contrast for Efficient Learning of Unsupervised" get accepted by <a href = "https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_AdCo_Adversarial_Contrast_for_Efficient_Learning_of_Unsupervised_Representations_From_CVPR_2021_paper.pdf" target="_blank">CVPR 2021</a>, the code and pre-trained model are also released.
                                <a href="https://github.com/mRepresentations from Self-Trained Negative Adversariesaple-research-lab/AdCo"><img alt="GitHub stars" src="https://img.shields.io/github/stars/maple-research-lab/AdCo?style=social"></a>
                                    </li>
                                    <br/>
                                </td>
                              </tr>
                            </table>



                            <! –– PySlowFast Release––>
                            <table>
                            <tr>
                                <td align="left" bgcolor="" width = "1000px" >
                                    <li> <b>Nov 2019</b>: Released <a href="https://github.com/facebookresearch/SlowFast">PySlowFast</a> codebase for video understanding research.&nbsp;<a href="https://github.com/facebookresearch/SlowFast"><img alt="GitHub stars" src="https://img.shields.io/github/stars/facebookresearch/SlowFast?style=social"></a></li>
                                    <br/>
                                </td>
                              </tr>
                            </table>

                            <! –– ICCV Tutorial––>
                            <table>
                            <tr>
                                <td align="left" bgcolor="" width = "1000px" >
                                    <li> <b>Nov 2019</b>: Co-organized a tutorial on <a href = "https://alexander-kirillov.github.io/tutorials/visual-recognition-iccv19/" target="_blank">Images, Video, and 3D</a> research and code at ICCV 2019.</li>
                                    <br/>
                                </td>
                              </tr>
                            </table>

                            <! –– AVA Win––>
                            <table>
                            <tr>
                                <td align="left" bgcolor="" width = "1000px" >
                                    <li> <b>June 2019</b>: Won the 1st place of <a href = "http://research.google.com/ava/challenge.html" target="_blank">AVA video activity detection challenge</a> at the <a href = "http://activity-net.org/challenges/2019/" target="_blank">International Challenge on Activity Recognition (ActvityNet) ICCV 2019</a>.</li>
                                    <br/>
                                </td>
                              </tr>
                            </table>

                            <! –– Outstanding Reviewer––>
                            <table>
                            <tr>
                                <td align="left" bgcolor="" width = "1000px" >
                                    <li> <b>June 2019</b>: Selected as CVPR 2019 <a href = "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954223" target="_blank">Outstanding Reviewer</a>.</li>
                                    <br/>
                                </td>
                              </tr>
                            </table>


                            <h3>Selected Publications</h3>


                            <! –– MViT ––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px">
                                    <img class="image" src="image/mvit-img-det-vid.png" width = "250px" alt="" />
                                </td>
                                <td align="left" bgcolor="" width = "600px" >

                                    <b>Multiscale Vision Transformers</b> <br/>
                                    <b>Haoqi Fan</b>, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer<br/>
                                    <i>ICCV</i>, 2021<br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2104.11227" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/facebookresearch/SlowFast" target="_blank" rel="noopener">
                                        <i class="fas fa-code mr-1"></i>
                                        code
                                    </a>
                                    <br/>

                                    <b>Improved Multiscale Vision Transformers for Classification and Detection</b> <br/>
                                    Yanghao Li, Chao-Yuan Wu, <b>Haoqi Fan</b>, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer<br/>
                                    <i>Tech Report</i>, 2021<br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2112.01526" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/facebookresearch/SlowFast" target="_blank" rel="noopener">
                                        <i class="fas fa-code mr-1"></i>
                                        code
                                    </a>
                                    <br/>



                                </td>
                              </tr>
                            </table>



                            <! –– MoCo Video ––>
                            <!--
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px">
                                    <img class="image" src="image/videomoco.png" width = "250px" alt="" />
                                    <img class="image" src="image/videomoco_output.png" width = "250px" alt="" />
                                </td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</b> <br/>
                                    Christoph Feichtenhofer, <b>Haoqi Fan</b>, Bo Xiong, Ross Girshick, Kaiming He<br/>
                                    <i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2021<br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2104.14558" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>
                            -->


                            <!--
                            <! –– Collaborative Mem ––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/c_mem.png" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>Beyond Short Clips: End-to-End Video-Level Learning with Collaborative Memories</b> <br/>
                                    Xitong Yang, <b>Haoqi Fan</b>, Lorenzo Torresani, Larry Davis, Heng Wang <br/>
                                    <i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2021<br/>
                                     <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2104.01198" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>


                            <! –– multiview video––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/multiview_video.png" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>Multiview Pseudo-Labeling for Semi-supervised Learning from Video</b> <br/>
                                    Bo Xiong, <b>Haoqi Fan</b>, Kristen Grauman, Christoph Feichtenhofer<br/>
                                    <i>Tech Report</i>, 2021<br/>
                                     <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2104.00682" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>
                            -->






                            <! –– MoCo v2 ––>
                            <!--
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/moco_v2.png" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>Improved Baselines with Momentum Contrastive Learning</b> <br/>
                                    Xinlei Chen, <b>Haoqi Fan</b>, Ross Girshick, Kaiming He <br/>
                                    <i>2-Page Tech Report</i>, 2020<br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://arxiv.org/abs/2003.04297" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/facebookresearch/moco" target="_blank" rel="noopener">
                                        <i class="fas fa-code mr-1"></i>
                                        code
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>
                            -->



                            <! –– MoCo ––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/moco_v1.png" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>Momentum contrast for unsupervised visual representation learning</b> <br/>
                                    Kaiming He, <b>Haoqi Fan</b>, Yuxin Wu, Saining Xie, Ross Girshick <br/>
                                    <i>Conference on Computer Vision and Pattern Recognition</i> (CVPR), 2020 (Oral)<br/>
                                    <i style="color:red">Best Paper Nomination</i><br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/1911.05722" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/facebookresearch/moco" target="_blank" rel="noopener">
                                        <i class="fas fa-code mr-1"></i>
                                        code
                                    </a>
                                    <br/>
                                    <b>Improved Baselines with Momentum Contrastive Learning</b> <br/>
                                    Xinlei Chen, <b>Haoqi Fan</b>, Ross Girshick, Kaiming He <br/>
                                    <i>2-Page Tech Report</i>, 2020<br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://arxiv.org/abs/2003.04297" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/facebookresearch/moco" target="_blank" rel="noopener">
                                        <i class="fas fa-code mr-1"></i>
                                        code
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>

                           
                            <! –– SlowFast ––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/slowfast.gif" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>Slowfast networks for video recognition</b> <br/>
                                    Christoph Feichtenhofer, <b>Haoqi Fan</b>, Jitendra Malik, Kaiming He <br/>
                                    <i>International Conference on Computer Vision</i> (ICCV), 2019 (Oral)<br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/1812.03982" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/facebookresearch/SlowFast" target="_blank" rel="noopener">
                                        <i class="fas fa-code mr-1"></i>
                                        code
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>


 
                            <! –– OctConv––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/octconv.png" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution</b> <br/>
                                    Yunpeng Chen, <b>Haoqi Fan</b>, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus Rohrbach, Shuicheng Yan, Jiashi Feng <br/>
                                    <i>International Conference on Computer Vision</i> (ICCV), 2019 <br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/1904.05049" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/facebookresearch/OctConv" target="_blank" rel="noopener">
                                        <i class="fas fa-code mr-1"></i>
                                        code
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>

                             
                            <!--
                            <! –– 3D dataset––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/3d_dataset.png" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>Order-Aware Generative Modeling Using the 3D-Craft Dataset</b> <br/>
                                    Zhuoyuan Chen∗ , Demi Guo∗ , Tong Xiao∗ , Saining Xie, Xinlei Chen, Haonan Yu, Jonathan Gray, Kavya Srinet, <b>Haoqi Fan</b>, Jerry Ma, Charles R. Qi, Shubham Tulsiani, Arthur Szlam, and C. Lawrence Zitnick <br/>
                                    <i>International Conference on Computer Vision</i> (ICCV), 2019 <br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Order-Aware_Generative_Modeling_Using_the_3D-Craft_Dataset_ICCV_2019_paper.pdf" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>
                            -->


 
                            <! –– LFB ––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/lfb.png" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>Long-Term Feature Banks for Detailed Video Understanding</b> <br/>
                                    Chao-Yuan Wu, Christoph Feichtenhofer, <b>Haoqi Fan</b>, Kaiming He, Philipp Krähenbühl, Ross Girshick <br/>
                                    <i>Conference on Computer Vision and Pattern Recognition</i> (CVPR), 2019 (Oral)<br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/1812.05038" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/facebookresearch/video-long-term-feature-banks" target="_blank" rel="noopener">
                                        <i class="fas fa-code mr-1"></i>
                                        code
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>

 
                            <! –– sla––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/sla.png" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>Stacked Latent Attention for Multimodal Reasoning</b> <br/>
                                    <b>Haoqi Fan</b>, Jiatong Zhou <br/>
                                    <i>Conference on Computer Vision and Pattern Recognition</i> (CVPR), 2018 <br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Stacked_Latent_Attention_CVPR_2018_paper.pdf" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>

                            <!--
                            <! –– k shot––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/k_shot.png" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>Efficient K-Shot Learning with Regularized Deep Networks</b> <br/>
                                    Donghyun Yoo, <b>Haoqi Fan</b>, Vishnu Naresh Boddeti, Kris M. Kitani <br/>
                                    <i>Thirty-Second AAAI Conference on Artificial Intelligence</i> (AAAI), 2018 <br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/1710.02277" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>
                            --> 

                            <! –– ego––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/ego.png" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>Going deeper into first-person activity recognition</b> <br/>
                                    Minghuang Ma, <b>Haoqi Fan</b>, Kris M. Kitani <br/>
                                    <i>Conference on Computer Vision and Pattern Recognition</i> (CVPR), 2016 <br/>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ma_Going_Deeper_into_CVPR_2016_paper.pdf" target="_blank" rel="noopener">
                                        <i class="fas fa-file-pdf mr-1"></i> Paper
                                    </a>
                                    <br/>
                                </td>
                              </tr>
                            </table>


                            <h3>Open Source Projects</h3>

                            <! –– PySlowFast––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img class="image" src="image/pyslowfast.gif" width = "250px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>PySlowFast: video understanding codebase for state-of-the-art research</b> <br/>
                                    <b>Haoqi Fan</b>, Yanghao Li, Wan-Yen Lo, Christoph Feichtenhofer<br/>

                                    <a href="https://github.com/facebookresearch/SlowFast"><img alt="GitHub stars" src="https://img.shields.io/github/stars/facebookresearch/SlowFast?style=social"></a></li>
                                    <br/>
                                </td>
                              </tr>
                            </table>
                            <! –– PyTorchVideo ––>
                            <table>
                            <tr>
                                <td align="left" width = "50px"></td>
                                <td align="left" width = "350px"><img style="margin-left:3.5em" class="image" src="image/ptv-teaser.gif" width = "125px" alt="" /></td>
                                <td align="left" bgcolor="" width = "600px" >
                                    <b>PyTorchVideo: A Deep Learning Library for Video Understanding</b> <br/>
                                    <b>Haoqi Fan *</b>, Tullie Murrell *, <br> Heng Wang <span>&#8225;</span>, Kalyan Vasudev Alwala<span>&#8225;</span>, Yanghao Li<span>&#8225;</span>, Yilei Li<span>&#8225;</span>, Bo Xiong <span>&#8225;</span>, <br> Nikhila Ravi, Meng Li, Haichuan Yang, Jitendra Malik, Ross Girshick, Matt Feiszli, <br> Aaron Adcock<span>&#8224;</span>, Wan-Yen Lo<span>&#8224;</span>, Christoph Feichtenhofer <span>&#8224;</span><br/>

                                    <a href="https://github.com/facebookresearch/pytorchvideo"><img alt="GitHub stars" src="https://img.shields.io/github/stars/facebookresearch/pytorchvideo?style=social"></a></li>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2111.09887?context=cs.LG" target="_blank" rel="noopener">
                                         <i class="fas fa-file-pdf mr-1"></i> Paper
                                     </a>
                                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ai.facebook.com/blog/pytorchvideo-a-deep-learning-library-for-video-understanding/" target="_blank" rel="noopener">
                                         <i class="fas fa-file-pdf mr-1"></i> Post
                                     </a>


                                    <br/>
                                </td>
                              </tr>
                            </table>





                    </div>
                </div>
            </div>
        </div>

        <div id="footer">

        </div>



  <div class="container">
    <footer class="site-footer">


  <p class="powered-by">

    © 2021 - 2022 · Xiao Wang




    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>

  </p>
</footer>

  </div>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-88566694-1', 'auto');
  ga('send', 'pageview');

</script>        


<script src="js/jquery.min.js"></script>
<script src="js/tether.min.js"></script>
<script src="js/bootstrap.min.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-89924514-1', 'auto');
  ga('send', 'pageview');

</script>

<script type="text/javascript" src="//rf.revolvermaps.com/0/0/7.js?i=5no0il9epuq&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;sx=0" async="async"></script>


    </body>
</html>
